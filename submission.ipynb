{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, default_data_collator, TrOCRProcessor\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import load_metric\n",
    "\n",
    "from sequence_mnist.model import SequenceMNIST\n",
    "from tests.test_sequence_mnist import test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed', num_labels=10)\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\n",
    "train_dataset = SequenceMNIST(train=True, processor=processor, root=\"/tmp/data\", download=True)\n",
    "test_dataset = SequenceMNIST(train=False, processor=processor, root=\"/tmp/data\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# Vocab size is the number of integers 0-9\n",
    "model.config.vocab_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    output_dir=\"results/\",\n",
    "    logging_steps=2,\n",
    "    save_steps=1000,\n",
    "    num_train_epochs=2,\n",
    "    eval_steps=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HUGGING FACE\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, pixel_values, **kwargs):\n",
    "    generated_ids = model.generate(pixel_values.unsqueeze(dim=0))\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(f'Prediction: {generated_text}, Sample: {kwargs}')\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a single instance\n",
    "predict(model=model, **test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'results/mnist_trained_trocr.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity metric isn't perfect as it doesn't account for order - but good enough proxy for quick eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_similarity(base_string: str, comp_string: str) -> float:\n",
    "    base_dict = dict(Counter(base_string))\n",
    "    for key in base_dict:\n",
    "        comp_count = comp_string.count(key)\n",
    "        gt_count = base_dict[key]\n",
    "        base_dict[key] -= comp_count if comp_count <= gt_count else gt_count\n",
    "\n",
    "    return 1 - sum(base_dict.values()) / (len(base_string) + 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(pred: str, gt: str) -> dict:\n",
    "\n",
    "    correct = True if pred == gt else False\n",
    "    similarity = string_similarity(base_string=gt, comp_string=pred) if len(pred) > 0 else 0.\n",
    "\n",
    "    return {'Correct' : correct, 'Similarity' : similarity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "for sample in test_dataset:\n",
    "    pred = predict(model=model, pixel_values=sample['pixel_values'])\n",
    "    gt = sample['text']\n",
    "    result = eval_metrics(pred, gt)\n",
    "    result['Prediction'] = pred\n",
    "    result['Ground Truth'] = gt\n",
    "    test_results.append(result)\n",
    "    #print(f'Label: {gt}, Prediction: {pred}, Result: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = pd.DataFrame(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counter = Counter()\n",
    "for sample in train_dataset:\n",
    "    train_counter.update(sample['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(train_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.hstack([torch.Tensor(img) for img in train_dataset.data[0:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out the images\n",
    "try:\n",
    "    i += 1\n",
    "except:\n",
    "    i = 0\n",
    "\n",
    "plt.imshow(train_dataset.data[i], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df.groupby(['Correct', 'Similarity']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df[(test_results_df['Similarity'] == 0.0) & (test_results_df['Prediction'] != '')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All similarity 0 results were sequences for which there was no prediction. If a prediction was made it is likely to be very similar (4/5 letters or 0.8 similarity) 490/573 = 86%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting edge case, spaces sometimes detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df[(test_results_df['Correct'] == False) & (test_results_df['Similarity'] == 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum(test_results_df['Correct'] == True) / len(test_results_df)\n",
    "print(f'Complete Match Accuracy: {accuracy*100:.1f}%')\n",
    "avg_sim = test_results_df['Similarity'].mean()\n",
    "print(f'Mean Similarity Measure: {avg_sim:.2f}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e286212fd47e27b247458097a4cf338d72a49115b1e2eb6aee25c4d5dedec74e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 ('huggingface')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
